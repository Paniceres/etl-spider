AquÃ­ tienes el **README.md** actualizado para **Agente Spider**, optimizado para enfocarse en la generaciÃ³n de CSVs crudos y alineado con la estructura y estilo del proyecto GOSOM, pero adaptado a las necesidades especÃ­ficas de Spider:

---

# ğŸš€âœ¨ Agente Spider - EspecificaciÃ³n de la Interfaz Streamlit ğŸ“ŠğŸ¤–

---

## ğŸ¯ Objetivo General de la UI

Proporcionar una **interfaz web simple y amigable** para configurar, ejecutar y monitorear tareas de scraping de Google Maps con el **Agente Spider**, generando **CSVs crudos por ciudad** que se procesarÃ¡n en el **ETL Central**.

---

## ğŸ“ Layout General

La interfaz se divide en dos Ã¡reas principales:

- ğŸ–¥ï¸ **Barra Lateral (Sidebar):**  
  - ConfiguraciÃ³n de parÃ¡metros (ciudades, keywords, profundidad, extracciÃ³n de emails).  
  - BotÃ³n "ğŸš€ Iniciar Scraping".  

- ğŸ“Š **Ãrea Principal (Main Area):**  
  - PestaÃ±as para:  
    - **Datos Crudos**: Vista previa y descarga por ciudad.  
    - **Logs**: VisualizaciÃ³n de registros de ejecuciÃ³n.  

---

## ğŸ› ï¸ Componentes y Funcionalidades Detalladas

### 1. âš™ï¸ ConfiguraciÃ³n de la Tarea de Scraping (Sidebar)

Permite al usuario definir los parÃ¡metros para ejecutar el scraping de Google Maps.

- âœ¨ **TÃ­tulo:** "Configurar Nueva Tarea de Scraping"  
- ğŸŒ **Widget 1: SelecciÃ³n de Ciudad(es)**  
  - **Tipo:** `st.multiselect`  
  - **Opciones:** Cargadas desde `SPIDER_COORDINATES` en `parameters_default.json`.  
  - **Label:** "Seleccionar Ciudad(es) a Procesar"  

- ğŸ“ **Widget 2: GestiÃ³n de Keywords por Ciudad**  
  - **Cada ciudad tiene un `st.text_area`** con keywords precargadas desde `/config/keywords_<ciudad>.csv`.  
  - **Editable:** El usuario puede aÃ±adir, modificar o eliminar palabras clave.  
  - **Label:** "Keywords para [Nombre de la Ciudad]"  

- ğŸ” **Widget 3: Profundidad de BÃºsqueda (`depth`)**  
  - **Tipo:** `st.slider`  
  - **Rango sugerido:** 1 a 20  
  - **Valor por defecto:** `DEFAULT_DEPTH` en `parameters_default.json`.  

- ğŸ“§ **Widget 4: Activar ExtracciÃ³n de Emails**  
  - **Tipo:** `st.checkbox`  
  - **Valor por defecto:** `True`  
  - **Nota:** Aumenta el tiempo de ejecuciÃ³n.  

- â–¶ï¸ **Widget 5: BotÃ³n de Inicio**  
  - **Tipo:** `st.button`  
  - **Label:** "ğŸš€ Iniciar Scraping"  

---

### 2. ğŸ“ˆ Monitoreo de Progreso (Main Area)

Mantiene informado al usuario sobre el estado de la ejecuciÃ³n.

- âœ¨ **TÃ­tulo:** "Progreso de la Tarea"  
- â³ **Indicador de Actividad:** `st.spinner("Scraping en progreso...")`  
- ğŸ“œ **Registro del Proceso (Log):**  
  - **ImplementaciÃ³n actual:** Mostrar contenido de `/data/logs/spider.log` con `st.text_area`.  
  - *Futuro:* Mostrar logs en tiempo real con `tail -n 50`.  

---

### 3. ğŸ“Š VisualizaciÃ³n de Resultados (Main Area - PestaÃ±as)

#### ğŸ“ PestaÃ±a/SecciÃ³n 1: Datos Crudos (por ciudad)

- Seleccionar ciudad de entre las procesadas.  
- Mostrar vista previa con `st.dataframe`.  
- Descargar CSV especÃ­fico con `st.download_button`.  

---

## ğŸ“‚ Estructura de Archivos

```text
0_agente_spider/
â”œâ”€â”€ 0_prompts/
â”‚   â”œâ”€â”€ 0_Futuro.md          # Ideas futuras
â”‚   â”œâ”€â”€ 1_Mejoras.md         # Tareas pendientes
â”‚   â””â”€â”€ 2_Historial.md       # Tareas completadas
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ parameters_default.json
â”‚   â””â”€â”€ keywords_<ciudad>.csv
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # CSVs crudos generados por Spider
â”‚   â””â”€â”€ logs/                # Logs de ejecuciÃ³n
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core_logic.py        # LÃ³gica de scraping (sin consolidaciÃ³n)
â”‚   â””â”€â”€ utils.py             # Funciones auxiliares
â”œâ”€â”€ app_streamlit.py         # Interfaz principal
â”œâ”€â”€ 0_PLAN_SPIDER.md         # PlanificaciÃ³n del proyecto
â””â”€â”€ README.md                # DocumentaciÃ³n general
```

---

## ğŸ“ Ubicaciones de Archivos Clave

- **CSVs Crudos:** `/data/raw/<ciudad>_<timestamp>.csv`  
- **ConfiguraciÃ³n de ParÃ¡metros:** `/config/parameters_default.json`  
- **Keywords por Ciudad:** `/config/keywords_<ciudad>.csv`  
- **Logs de EjecuciÃ³n:** `/data/logs/spider.log`  

---

## ğŸš¶â€â™‚ï¸ Flujo de Usuario TÃ­pico

1. **Acceso:**  
   - El usuario accede a la aplicaciÃ³n Streamlit desde el navegador.  

2. **ConfiguraciÃ³n:**  
   - En la barra lateral:  
     - Selecciona ciudades y edita keywords.  
     - Define profundidad y activa/desactiva extracciÃ³n de emails.  

3. **EjecuciÃ³n:**  
   - Hace clic en "ğŸš€ Iniciar Scraping".  
   - Mientras se ejecuta:  
     - Ve un spinner indicando actividad.  
     - Al finalizar, muestra el log completo del proceso.  

4. **Resultados:**  
   - Descarga los CSVs crudos generados.  

---

## ğŸ“‚ DocumentaciÃ³n Adicional

- ğŸš€ **[Ideas Futuras](0_prompts/0_Futuro.md):** Mejoras para versiones futuras.  
- ğŸ“‹ **[Tareas Pendientes](0_prompts/1_Mejoras.md):** Plan de trabajo para el MVP.  
- âœ… **[Historial de Mejoras Completadas](0_prompts/2_Historial.md):** Registro de tareas resueltas.  

---

## ğŸ“Œ Notas Clave

- **Enfoque en Scraping:** Spider se limita a generar CSVs crudos sin procesamiento adicional.  
- **ETL Central:** Toda la consolidaciÃ³n, deduplicaciÃ³n y chunkeo se realizarÃ¡n en un mÃ³dulo separado.  
- **Sin dependencias costosas:** Solo herramientas gratuitas (Streamlit, Pandas).  

---

*Â¡Gracias por leer esta documentaciÃ³n!  
El Agente Spider estÃ¡ diseÃ±ado para ser **simple, modular y escalable**.* ğŸŒŸ  

---
