# ğŸ“ 0_PLAN_SPIDER.md
---
## ğŸ¯ Objetivo General  
Crear una **UI Streamlit** gratuita y profesional que permita:  
- Definir **keywords por ciudad**.  
- Ejecutar pipelines de scraping con **Spider-py/Rs**.  
- Descargar CSVs â€œrawâ€ por ciudad.  

---
## ğŸ§° MÃ³dulos y Tareas a Desarrollar  

### 1. DocumentaciÃ³n y PlanificaciÃ³n  
- [x] ğŸ“„ **README.md** base (ya hecho)  
- [x] ğŸ“‹ **0_PLAN_SPIDER.md** (este archivo)  
- [x] ğŸ“ Plantillas de Prompts para Gemini: **/0_AGENTE_SPIDER/0_prompts/0_Futuro.md**, **/0_AGENTE_SPIDER/0_prompts/0_Pasado.md**, **/0_AGENTE_SPIDER/0_prompts/1_Mejoras.md**
  - Prompt 1: Analizar errores en **/0_AGENTE_SPIDER/app_streamlit.py** y actualizar **/0_AGENTE_SPIDER/0_prompts/1_Mejoras.md**.
  - Prompt 2: Ejecutar tareas de mejora y mover a **/0_AGENTE_SPIDER/0_prompts/0_Pasado.md**.

### 2. LÃ³gica Central (`core_logic.py`)
- [x] âš™ï¸ `run_spider(config: dict) â†’ pd.DataFrame`: ImplementaciÃ³n simulada aÃ±adida. Requiere integraciÃ³n con Spider-py/Rs.
- [x] ğŸ•¸ï¸ `parse_html(html: str, rules: dict) â†’ dict`: ImplementaciÃ³n detallada aÃ±adida usando BeautifulSoup para extraer datos segÃºn reglas.
- [x] ğŸ“œ Configurar **RotatingFileHandler** para `/logs/spider.log`. Se ha aÃ±adido cÃ³digo placeholder y se requiere implementaciÃ³n real.
- [x] AÃ±adir estructura detallada y comentarios a las funciones placeholder.
- [x] AÃ±adir funciones auxiliares: `generate_google_maps_urls` y `_get_parsing_rules`. La estructura para integrar `spider-py/Rs` en `run_spider` estÃ¡ definida, pero **requiere reemplazar la simulaciÃ³n de ejecuciÃ³n** y **ajustar los selectores de parsing** en `_get_parsing_rules` para que coincidan con la estructura real de Google Maps.

### 3. Interfaz Streamlit (`app_streamlit.py`)
- [x] ğŸ–¥ï¸ **Sidebar**  
  - [x] `st.multiselect(\"Seleccionar Ciudad(es)\", opciones)`
  - [x] `st.text_area(\"Keywords para X\", value=â€¦)` + BotÃ³n â€œGuardar keywordsâ€
  - [x] Mejorar manejo de errores, retroalimentaciÃ³n al usuario, y aÃ±adir indicador de progreso para el scraping.
  - [x] `st.checkbox` para extracciÃ³n de emails
  - [x] `st.button("ğŸš€ Iniciar Scraping")`
- [x] ğŸ“Š **Main Area con pestaÃ±as:**  

#### Refinamientos Adicionales de UI
- [ ] AÃ±adir mensajes de error visuales (alta prioridad)
- [ ] Validar permisos de escritura en `/data/` (alta prioridad)

  - [x] **Crudos**: previsualizaciÃ³n + `st.download_button` por ciudad
 - Se ha aÃ±adido un resumen bÃ¡sico del DataFrame (filas y columnas).
  - [x] **Logs**: `st.text_area` con tail de `/logs/spider.log`  
- [x] ğŸ§© Usar `st.expander()` y `st.columns()` para organizaciÃ³n  
- [x] GestiÃ³n de Keywords por Ciudad
### 4. ConfiguraciÃ³n y Datos
- [x] ğŸ—‚ï¸ /0_AGENTE_SPIDER/config/cities.json (Coordenadas de ciudades)
- [x] ğŸ—‚ï¸ Ejemplos de **/0_AGENTE_SPIDER/config/keywords_<ciudad>.csv**: **/0_AGENTE_SPIDER/config/keywords_example.csv**, **/0_AGENTE_SPIDER/config/keywords_neuquen.csv**, **/0_AGENTE_SPIDER/config/keywords_cipolletti.csv**
- [x] ğŸ“‚ **/0_AGENTE_SPIDER/data/raw/** (CSVs crudos generados por Spider): Estructura de directorios base creada. Se ha aÃ±adido cÃ³digo placeholder y se requiere implementaciÃ³n real.

## ğŸ“š Historial de Mejoras (Checklist Estilo GOSOM)


---

## ğŸ”® Siguientes Pasos  
 Lista de Tareas Pendientes para Agente Spider:

Estas tareas se centran principalmente en implementar la lÃ³gica de scraping real y refinar la interfaz de usuario.

IntegraciÃ³n de Spider-py/Rs en core_logic.py:

Objetivo: Reemplazar la implementaciÃ³n simulada de run_spider con llamadas reales a la librerÃ­a spider-py/Rs. Acciones: Modificar la funciÃ³n run_spider en agente_spider/src/core_logic.py. Utilizar spider-py/Rs para configurar y ejecutar las tareas de scraping basadas en el diccionario config (que incluye ciudades, keywords, profundidad y opciÃ³n de extraer emails). Asegurar que la funciÃ³n run_spider devuelva los resultados scrapeados como un pandas DataFrame. Manejar posibles errores o excepciones durante la ejecuciÃ³n de spider-py/Rs y registrarlos usando el sistema de logging. Ajuste de Selectores de Parsing en core_logic.py:

Objetivo: Asegurar que la funciÃ³n parse_html extraiga correctamente los datos de las pÃ¡ginas de resultados de Google Maps utilizando BeautifulSoup. Acciones: Revisar la funciÃ³n _get_parsing_rules y la lÃ³gica dentro de parse_html en agente_spider/src/core_logic.py. Actualizar los selectores CSS o XPath (dependiendo de cÃ³mo parse_html utilice BeautifulSoup y las "rules") para que coincidan con la estructura HTML actual de los listados de negocios en Google Maps. Identificar y extraer campos clave como nombre del negocio, categorÃ­a, direcciÃ³n, telÃ©fono, sitio web, email (si aplica y es extraÃ­ble), etc. ImplementaciÃ³n Real del Logging en core_logic.py:

Objetivo: Configurar y utilizar correctamente un RotatingFileHandler para registrar la actividad y los errores del proceso de scraping principal. Acciones: Asegurarse de que el logger en agente_spider/src/core_logic.py (o donde se maneje el logging del core) estÃ© configurado con RotatingFileHandler apuntando a agente_spider/data/logs/spider_core.log. AÃ±adir mensajes de log informativos, de advertencia y de error en puntos clave de la ejecuciÃ³n de run_spider y las funciones relacionadas. Guardar Datos Scrapeados en /data/raw/ desde core_logic.py:

Objetivo: Guardar automÃ¡ticamente los resultados del scraping en archivos CSV separados en el directorio agente_spider/data/raw/. Acciones: Dentro de run_spider (o una funciÃ³n auxiliar llamada por esta), implementar la lÃ³gica para guardar el DataFrame resultante (o partes de Ã©l, por ciudad o keyword) en archivos CSV. Utilizar pandas.DataFrame.to_csv para guardar los datos. Asegurar que los nombres de los archivos CSV sean descriptivos (ej: resultados_neuquen_abogados_YYYYMMDD_HHMMSS.csv). Manejar posibles errores de escritura de archivos. Refinamientos Adicionales de la UI en app_streamlit.py:

Objetivo: Mejorar la retroalimentaciÃ³n visual al usuario. Acciones: Implementar mensajes de error mÃ¡s visibles y amigables en la interfaz de Streamlit (utilizando st.error, st.warning, etc.) para los problemas que puedan ocurrir durante la carga de configuraciÃ³n, el scraping o el guardado de archivos. Validar los permisos de escritura en el directorio agente_spider/data/ al inicio de la aplicaciÃ³n o antes de intentar guardar archivos, e informar al usuario si hay problemas.