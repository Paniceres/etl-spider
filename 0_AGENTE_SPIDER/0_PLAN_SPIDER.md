# ğŸ“ 0_PLAN_SPIDER.md

---

## ğŸ¯ Objetivo General  
Crear una **UI Streamlit** gratuita y profesional que permita:  
- Definir **keywords por ciudad**.  
- Ejecutar pipelines de scraping con **Spider-py/Rs**.  
- Descargar CSVs â€œrawâ€ por ciudad.  
- (Manual) Consolidar esos CSVs en un **CSV Madre** (delegado a ETL Central).  
- (Opcional) Chunkear desde la UI si pinta (delegado a ETL Central).  

---

## ğŸ§° MÃ³dulos y Tareas a Desarrollar

<<<<<<< Updated upstream
### 1. DocumentaciÃ³n & PlanificaciÃ³n  
- [ ] ğŸ“„ **README.md** base (ya hecho)  
- [ ] ğŸ“‹ **0_PLAN_SPIDER.md** (este archivo)  
- [ ] ğŸ“ Plantillas de Prompts para Gemini:  
  - Prompt 1: Analizar errores en `app_streamlit.py` y actualizar `0_mejoras.md`.  
  - Prompt 2: Ejecutar tareas de mejora y mover a `0_pasado.md`.  
  - Prompt 3: Registrar tareas completadas en `4_registro_tareas.md`.  

### 2. LÃ³gica Central (`core_logic.py`)  
- [ ] âš™ï¸ `run_spider(config: dict) â†’ pd.DataFrame`  
- [ ] ğŸ•¸ï¸ `parse_html(html: str, rules: dict) â†’ dict`  
- [ ] ğŸ“œ Configurar **RotatingFileHandler** para `/logs/spider.log`  

### 3. Interfaz Streamlit (`app_streamlit.py`)  
- [ ] ğŸ–¥ï¸ **Sidebar**  
  - [ ] `st.multiselect("Seleccionar Ciudad(es)", opciones)`  
  - [ ] `st.text_area("Keywords para X", value=â€¦)` + BotÃ³n â€œGuardar keywordsâ€  
  - [ ] `st.slider` o `st.number_input` para `depth`  
  - [ ] `st.checkbox` para extracciÃ³n de emails  
  - [ ] `st.button("ğŸš€ Iniciar Scraping")`  
- [ ] ğŸ“Š **Main Area** con pestaÃ±as:  
  - [ ] **Crudos**: preview + `st.download_button` por ciudad  
  - [ ] **Logs**: `st.text_area` con tail de `/logs/streamlit.log`  
- [ ] ğŸ§© Usar `st.expander()` y `st.columns()` para organizaciÃ³n  

### 4. Config & Datos  
- [ ] ğŸ—‚ï¸ `/config/parameters_default.json` (ciudades, depth default)  
- [ ] ğŸ—‚ï¸ Ejemplos de `/config/keywords_<ciudad>.csv`  
- [ ] ğŸ“‚ `/data/raw/` (CSVs crudos generados por Spider)  
- [ ] ğŸ“‚ `/logs/` con `spider.log` y `streamlit.log` vacÃ­os  
- [ ] ğŸ”„ **IntegraciÃ³n con ETL Central:**  
  - Archivos crudos se guardan en `/data/raw/` para ser procesados por ETL Central.  
  - ETL Central se encargarÃ¡ de consolidaciÃ³n, deduplicaciÃ³n y chunkeo.  

### 5. Pruebas & CI  
- [ ] âœ… `tests/test_run_spider.py`  
- [ ] âœ… `tests/test_logs.py`  
- [ ] âš™ï¸ GitHub Actions: instalar deps + correr pytest + flake8  

### 6. Deploy (Opcional)  
- [ ] ğŸš€ `deploy.sh` para Streamlit Community Cloud o Heroku gratis  
- [ ] ğŸ“œ Instrucciones en README  

---
=======
### 2. LÃ³gica Central (`core_logic.py`)
- [x] âš™ï¸ `run_spider(config: dict) â†’ pd.DataFrame`: ImplementaciÃ³n simulada aÃ±adida. Requiere integraciÃ³n con Spider-py/Rs.
- [x] ğŸ•¸ï¸ `parse_html(html: str, rules: dict) â†’ dict`: ImplementaciÃ³n detallada aÃ±adida usando BeautifulSoup para extraer datos segÃºn reglas.
- [x] ğŸ“œ Configurar **RotatingFileHandler** para `/logs/spider.log`. Se ha aÃ±adido cÃ³digo placeholder y se requiere implementaciÃ³n real.
- [x] AÃ±adir estructura detallada y comentarios a las funciones placeholder.
- [x] AÃ±adir funciones auxiliares: `generate_google_maps_urls` y `_get_parsing_rules`. La estructura para integrar `spider-py/Rs` en `run_spider` estÃ¡ definida, pero **requiere reemplazar la simulaciÃ³n de ejecuciÃ³n** y **ajustar los selectores de parsing** en `_get_parsing_rules` para que coincidan con la estructura real de Google Maps.

### 3. Interfaz Streamlit (`app_streamlit.py`)
- [x] ğŸ–¥ï¸ **Sidebar**  
  - [x] `st.multiselect(\"Seleccionar Ciudad(es)\", opciones)`
  - [x] `st.text_area(\"Keywords para X\", value=â€¦)` + BotÃ³n â€œGuardar keywordsâ€
  - [x] Mejorar manejo de errores, retroalimentaciÃ³n al usuario, y aÃ±adir indicador de progreso para el scraping.
  - [x] `st.checkbox` para extracciÃ³n de emails
  - [ ] `st.button("ğŸš€ Iniciar Scraping")`
- [x] ğŸ“Š **Main Area con pestaÃ±as:**  

#### Refinamientos Adicionales de UI
- [ ] AÃ±adir mensajes de error visuales (alta prioridad)
- [ ] Validar permisos de escritura en `/data/` (alta prioridad)

  - [x] **Crudos**: previsualizaciÃ³n + `st.download_button` por ciudad
 - Se ha aÃ±adido un resumen bÃ¡sico del DataFrame (filas y columnas).
  - [x] **Logs**: `st.text_area` con tail de `/logs/spider.log`  
- [x] ğŸ§© Usar `st.expander()` y `st.columns()` para organizaciÃ³n  
- [x] GestiÃ³n de Keywords por Ciudad
### 4. ConfiguraciÃ³n y Datos
- [x] ğŸ—‚ï¸ /0_AGENTE_SPIDER/config/cities.json (Coordenadas de ciudades)
- [x] ğŸ—‚ï¸ Ejemplos de **/0_AGENTE_SPIDER/config/keywords_<ciudad>.csv**: **/0_AGENTE_SPIDER/config/keywords_example.csv**, **/0_AGENTE_SPIDER/config/keywords_neuquen.csv**, **/0_AGENTE_SPIDER/config/keywords_cipolletti.csv**
- [x] ğŸ“‚ **/0_AGENTE_SPIDER/data/raw/** (CSVs crudos generados por Spider): Estructura de directorios base creada. Se ha aÃ±adido cÃ³digo placeholder y se requiere implementaciÃ³n real.
>>>>>>> Stashed changes

## ğŸ“š Historial de Mejoras (Checklist Estilo GOSOM)

- [ ] **Fase 1: MVP**  
  - Interfaz bÃ¡sica, `run_spider`, descarga raw.  
- [ ] **Fase 2: IntegraciÃ³n con ETL Central**  
  - Validar que CSVs crudos se guardan en `/data/raw/` correctamente.  
- [ ] **Fase 3: QA & Docs**  
  - Tests, CI, docs finales.  

---

## ğŸ”® Siguientes Pasos  
1. Validar este **0_PLAN_SPIDER.md**.  
2. Empezar por **core_logic.py**: crear `run_spider()`.  
3. Avanzar en **app_streamlit.py** mÃ³dulo a mÃ³dulo.  
4. Configurar **prompts para Gemini** para automatizar tareas.  